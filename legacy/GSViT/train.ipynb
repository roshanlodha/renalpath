{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee40b8d6-ff04-4d22-bcca-d83e6099dc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lodhar2/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/lodhar2/.local/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import os\n",
    "os.chdir(\"/data/lodhar2/GSViT\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, CenterCrop, RandomCrop, Resize, RandomHorizontalFlip, RandomRotation, ToTensor\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from EfficientViT.classification.model.build import EfficientViT_M5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b84ef88c-820e-4436-a524-5836e2c779dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————\n",
    "# 1. channel-flip to match GSViT’s BGR convention\n",
    "def process_inputs(images):\n",
    "    tmp = images[:, 0, :, :].clone()\n",
    "    images[:, 0, :, :] = images[:, 2, :, :]\n",
    "    images[:, 2, :, :] = tmp\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348b5ce6-c685-43bd-9eac-9308ec25d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————\n",
    "# 2. Model wrapper with new head, filtering only the `evit.` keys\n",
    "class GSViTWithHead(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # build backbone and strip its old classifier\n",
    "        backbone_pre = EfficientViT_M5(pretrained=None)\n",
    "        backbone = nn.Sequential(*list(backbone_pre.children())[:-1])\n",
    "\n",
    "        # load checkpoint & keep only the \"evit.\" entries\n",
    "        ckpt = torch.load(\"GSViT.pkl\", map_location=\"cpu\")\n",
    "        filtered = {\n",
    "            k[len(\"evit.\"):]: v\n",
    "            for k,v in ckpt.items()\n",
    "            if k.startswith(\"evit.\")\n",
    "        }\n",
    "        backbone.load_state_dict(filtered, strict=False)\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # figure out feature‐dim on a dummy\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 224, 224)\n",
    "            feat = self.backbone(dummy)\n",
    "            feat_dim = feat.view(1, -1).shape[1]\n",
    "\n",
    "        # new classification head\n",
    "        self.classifier = nn.Linear(feat_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = process_inputs(x)\n",
    "        feats = self.backbone(x).view(x.size(0), -1)\n",
    "        return self.classifier(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddc5c75e-881e-45f0-b8e6-e43f6351890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——————————————\n",
    "# 3. Dataset → yields (image_tensor, label_idx)\n",
    "class HistologyDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(self.df[\"Class\"].unique())\n",
    "        self.class_to_idx = {c:i for i,c in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = os.path.join(self.root, row[\"img_path\"])\n",
    "        img  = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = self.class_to_idx[row[\"Class\"]]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8712eeb2-6a16-43ba-8724-52111bc875f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"split_balanced_dataset.csv\")\n",
    "#df['Class'] = np.where(df['Class'] == \"Clear_cell\", \"Clear_cell\", \"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db4be32-ee01-47de-9dca-c62b03c8f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"split\"]==\"train\"]\n",
    "val_df   = df[df[\"split\"]==\"val\"]\n",
    "\n",
    "# === Transforms ===\n",
    "train_transform = Compose([\n",
    "    CenterCrop(1080),\n",
    "    RandomCrop(768),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(degrees=15),\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    CenterCrop(768),\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "# === Dataloaders ===\n",
    "train_loader = DataLoader(\n",
    "    HistologyDataset(train_df, \".\", transform=train_transform),\n",
    "    batch_size=16, shuffle=True,  num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    HistologyDataset(val_df, \".\", transform=val_transform),\n",
    "    batch_size=16, shuffle=False, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f3976c7-c988-48d5-873d-664a623e35d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = GSViTWithHead(num_classes=len(train_df[\"Class\"].unique())).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1abda6c-db05-4100-be7e-80c017a4bbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 9.6432\n",
      "Epoch 1 val balanced acc: 0.1558\n",
      "saved new best model\n",
      "Epoch 2 train loss: 5.5558\n",
      "Epoch 2 val balanced acc: 0.2027\n",
      "saved new best model\n",
      "Epoch 3 train loss: 5.6478\n",
      "Epoch 3 val balanced acc: 0.1483\n",
      "Epoch 4 train loss: 6.5286\n",
      "Epoch 4 val balanced acc: 0.1653\n",
      "Epoch 5 train loss: 4.7867\n",
      "Epoch 5 val balanced acc: 0.1738\n",
      "Epoch 6 train loss: 6.3377\n",
      "Epoch 6 val balanced acc: 0.1780\n",
      "Epoch 7 train loss: 4.9505\n",
      "Epoch 7 val balanced acc: 0.1639\n",
      "Epoch 8 train loss: 4.8724\n",
      "Epoch 8 val balanced acc: 0.1863\n",
      "Epoch 9 train loss: 4.0978\n",
      "Epoch 9 val balanced acc: 0.1754\n",
      "Epoch 10 train loss: 4.0055\n",
      "Epoch 10 val balanced acc: 0.1812\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "for epoch in range(1, 11):\n",
    "    # — train —\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss   = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch} train loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # — validate —\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits = model(imgs)\n",
    "            preds  = logits.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # Flatten predictions and labels\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch} val balanced acc: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"best_gsvit.pth\")\n",
    "        print(\"saved new best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca944c-f07a-463c-afad-b0fecd51bf82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python/3.10",
   "language": "python",
   "name": "py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
